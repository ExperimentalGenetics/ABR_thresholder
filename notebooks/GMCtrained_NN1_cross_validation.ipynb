{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, a five-fold grouped cross-validation was performed for **Model I** with data from the [German Mouse Clinic](https://www.mouseclinic.de/),\n",
    "where the groups consist of mice.\n",
    "\n",
    "First, a 4:1 randomly split into training and test mice is done. The training data is then randomly split 4:1 into training and validation mice in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:36:09.317727Z",
     "start_time": "2021-02-03T16:36:09.041231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T09:12:48.030699Z",
     "start_time": "2021-02-24T09:12:47.998242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 GPUS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 16]\n",
    "\n",
    "import ABR_ThresholdFinder_NN.data_preparation as dataprep\n",
    "from ABR_ThresholdFinder_NN.models import create_model_1, compile_model_1\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\"Define available GPUs\"\"\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "count_gpus = len([int(s) for s in os.environ['CUDA_VISIBLE_DEVICES'].split(',')])\n",
    "print('%d GPUS' % count_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:36:17.586073Z",
     "start_time": "2021-02-03T16:36:17.519937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 64\n",
      "potential stimulus frequencies: 100, 6000Hz, 12000Hz, 18000Hz, 24000Hz, 30000Hz\n",
      "potential sound pressure levels [dB]: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set the batch size\"\"\"\n",
    "batch_size=64\n",
    "print('batch size: {}'.format(batch_size))\n",
    "\n",
    "\"\"\"Initialize the sample weight column (for the main label) - if necessary this will be filled later on\"\"\"\n",
    "sample_weight_col = []\n",
    "\n",
    "\"\"\"\n",
    "Define potential frequencies measured in Hz, with the exception of 100, \n",
    "which stands for a broadband frequency stimulus (click)  \n",
    "\"\"\"\n",
    "poss_freq = [100, 6000, 12000, 18000, 24000, 30000]\n",
    "print(*['potential stimulus frequencies: ' + str(x) if x==100 else str(x)+'Hz' for x in poss_freq], sep = \", \") \n",
    "\n",
    "\"\"\"Define potential sound pressure levels measured in dB\"\"\"\n",
    "poss_thr = [p for p in range(0, 100, 5)]\n",
    "print(*['potential sound pressure levels [dB]: ' + str(x) if x==0 else str(x) for x in poss_thr], sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:36:21.677532Z",
     "start_time": "2021-02-03T16:36:21.279199Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Set the path to the data files, for example '../data/GMC'\"\"\"\n",
    "data_dir = ''\n",
    "data_file = os.path.join(data_dir, 'GMC_abr_curves.csv')\n",
    "if os.path.exists(data_file):\n",
    "    print('data file: {}'.format(data_file))\n",
    "else: \n",
    "    print('ERROR: data file not found!')\n",
    "save_dir = '../models_cross-validation/GMC/cv_network_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:37:47.155433Z",
     "start_time": "2021-02-03T16:36:25.716226Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load data from file\"\"\"\n",
    "data = pd.read_csv(data_file)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:38:07.486540Z",
     "start_time": "2021-02-03T16:38:01.602463Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Prepare the data for the cross validation approach\"\"\"\n",
    "data1, sample_weight_col = dataprep.prepare_data4training_1(data, poss_freq, poss_thr)\n",
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:38:32.512714Z",
     "start_time": "2021-02-03T16:38:32.450844Z"
    }
   },
   "outputs": [],
   "source": [
    "main_label_col, freq_label_col, sl_label_col, data_cols = dataprep.get_col_names4model_1_labels(data1)\n",
    "print('main label column: %s' % main_label_col)\n",
    "print('frequency label columns: [%s ... %s]' % (freq_label_col[0], freq_label_col[-1]))\n",
    "print('sound level label columns: [%s ... %s]' % (sl_label_col[0], sl_label_col[-1]))\n",
    "print('data columns: [%s ... %s]' % (data_cols[0], data_cols[-1]))\n",
    "\n",
    "print('sample weight column: %s' % sample_weight_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "print('random seed: {}'.format(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mice, test_mice, train_indices, test_indices = dataprep.split_data(data2, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:39:07.522358Z",
     "start_time": "2021-02-03T16:39:07.428931Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mice = np.load(os.path.join(data_dir, 'GMC_train_mice4cross_validation.npy'))\n",
    "test_mice = np.load(os.path.join(data_dir, 'GMC_test_mice.npy'))\n",
    "\n",
    "train_indices = data1.index[data1['mouse_id'].isin(train_mice)]\n",
    "test_indices = data1.index[data1['mouse_id'].isin(test_mice)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:39:22.654496Z",
     "start_time": "2021-02-03T16:39:22.611845Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:39:24.766414Z",
     "start_time": "2021-02-03T16:39:24.712471Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:39:27.058916Z",
     "start_time": "2021-02-03T16:39:27.003465Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_weights_name(k):\n",
    "    return 'model_'+str(k)+'weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:39:30.986869Z",
     "start_time": "2021-02-03T16:39:30.928122Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Import data generator\"\"\"\n",
    "from ABR_ThresholdFinder_NN.data_generator import DataGenerator\n",
    "\"\"\"Import callbacks\"\"\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:42:32.657611Z",
     "start_time": "2021-02-03T16:41:38.411016Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Perform the cross validation for Model I\"\"\"\n",
    "VAL_RESULTS = {}\n",
    "\n",
    "fold_var = 1\n",
    "mon_var = 'val_main_prediction_loss'\n",
    "\n",
    "for train_index, val_index in kf.split(train_mice):\n",
    "    \n",
    "    print('fold %d' % fold_var)\n",
    "    \n",
    "    _train_mice = train_mice[train_index]\n",
    "    _val_mice = train_mice[val_index]\n",
    "    print('overlap train/validation: %s' % np.intersect1d(_train_mice, _val_mice))\n",
    "    \n",
    "    train_index = data1.index[data1['mouse_id'].isin(_train_mice)].values\n",
    "    val_index = data1.index[data1['mouse_id'].isin(_val_mice)].values\n",
    "    \n",
    "    print(train_index, val_index)\n",
    "    print()\n",
    "\n",
    "    data2 = dataprep.standardize(data1, train_index, data_cols)\n",
    "\n",
    "    \"\"\"Define data generators (actually keras sequences)\"\"\"\n",
    "    train_data_generator = DataGenerator(list_IDs=train_index,\n",
    "                                         df=data2,\n",
    "                                         value_cols=data_cols,\n",
    "                                         main_label_col=main_label_col,\n",
    "                                         freq_label_col=freq_label_col,\n",
    "                                         sl_label_col=sl_label_col,\n",
    "                                         dim=1000,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         sample_weight_col=sample_weight_col)\n",
    "    valid_data_generator = DataGenerator(list_IDs=val_index,\n",
    "                                         df=data2,\n",
    "                                         value_cols=data_cols,\n",
    "                                         main_label_col=main_label_col,\n",
    "                                         freq_label_col=freq_label_col,\n",
    "                                         sl_label_col=sl_label_col,\n",
    "                                         dim=1000,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         sample_weight_col=sample_weight_col)\n",
    "\n",
    "    \"\"\"Create Model I\"\"\"\n",
    "    model = create_model_1(len(freq_label_col), len(sl_label_col))\n",
    "    # COMPILE NEW MODEL\n",
    "    parallel_model, loss, loss_weights = compile_model_1(model, count_gpus)\n",
    "\n",
    "    \"\"\"Create callbacks\"\"\"\n",
    "    checkpoint = ModelCheckpoint(filepath=save_dir+get_model_weights_name(fold_var),\n",
    "                                 verbose=1, save_best_only=True, monitor=mon_var)\n",
    "    early_stopper = EarlyStopping(monitor=mon_var, patience=7)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=mon_var, patience=2)\n",
    "    callbacks_list = [checkpoint, early_stopper, reduce_lr]\n",
    "\n",
    "    \"\"\"Fit the model\"\"\"\n",
    "    history = parallel_model.fit_generator(generator=train_data_generator,\n",
    "                                           validation_data=valid_data_generator,\n",
    "                                           use_multiprocessing=True,\n",
    "                                           epochs=30,\n",
    "                                           workers=8,\n",
    "                                           shuffle=True,\n",
    "                                           callbacks=callbacks_list)\n",
    "    \n",
    "    \"\"\"Plot history\"\"\"\n",
    "    \"\"\"Summarize history for loss\"\"\"\n",
    "    plt.figure(figsize=(20, 14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(history.history['main_prediction_loss'])\n",
    "    plt.plot(history.history['val_main_prediction_loss'])\n",
    "    plt.title(get_model_name(fold_var) + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.savefig(save_dir + \"/model_\" + str(fold_var) + \"_loss_history_.png\")\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"Summarize history for accuracy\"\"\"\n",
    "    plt.figure(figsize=(20, 14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(history.history['main_prediction_acc'])\n",
    "    plt.plot(history.history['val_main_prediction_acc'])\n",
    "    plt.title(get_model_name(fold_var) + ' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='lower right')\n",
    "    plt.savefig(save_dir + \"/model_\" + str(fold_var) + \"_acc_history_.png\")\n",
    "\n",
    "    \"\"\"Load the best model to evaluate the model performance\"\"\"\n",
    "    parallel_model.load_weights(save_dir + get_model_weights_name(fold_var))\n",
    "    model.save(save_dir + get_model_name(fold_var))\n",
    "\n",
    "    results = parallel_model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(parallel_model.metrics_names,results))\n",
    "\n",
    "    VAL_RESULTS[get_model_name(fold_var)] = results\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    fold_var += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['model'])\n",
    "for idx,model in enumerate(VAL_RESULTS):\n",
    "    df_results.loc[idx, 'model'] = model \n",
    "    for metric in VAL_RESULTS[model]:\n",
    "        df_results.loc[idx, metric] = VAL_RESULTS[model][metric]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss: %.2f' % df_results.loss.mean())\n",
    "print(' main prediction loss: %.2f' % df_results.main_prediction_loss.mean())\n",
    "print(' frequency prediction loss: %.2f' % df_results.frequency_prediction_loss.mean())\n",
    "print(' sound level prediction loss: %.2f' % df_results.sl_prediction_loss.mean())\n",
    "print()\n",
    "print('main prediction accuracy: %.2f' % df_results.main_prediction_acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_generator = DataGenerator(list_IDs=test_indices, \n",
    "                                    df=data3, \n",
    "                                    value_cols=data_cols, \n",
    "                                    main_label_col=main_label_col,\n",
    "                                    freq_label_col=freq_label_col, \n",
    "                                    sl_label_col=sl_label_col,\n",
    "                                    dim=1000, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True,\n",
    "                                    sample_weight_col=sample_weight_col)\n",
    "TST_RESULTS = {}\n",
    "for fold_var in range(1, 6):\n",
    "    \n",
    "    print(fold_var)\n",
    "    \n",
    "    \"\"\"Load model for testing\"\"\"\n",
    "    parallel_model = tf.keras.models.load_model(save_dir + get_model_weights_name(fold_var))\n",
    "   \n",
    "    \"\"\"Evaluate model on test data set\"\"\"    \n",
    "    results = parallel_model.evaluate(test_data_generator)\n",
    "    results = dict(zip(parallel_model.metrics_names,results))\n",
    "    \n",
    "    TST_RESULTS[get_model_name(fold_var)] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results1 = pd.DataFrame(columns=['model'])\n",
    "for idx,model in enumerate(TST_RESULTS):\n",
    "    df_results1.loc[idx, 'model'] = model \n",
    "    for metric in TST_RESULTS[model]:\n",
    "        df_results1.loc[idx, metric] = TST_RESULTS[model][metric]\n",
    "df_results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss: %.2f' % df_results1.loss.mean())\n",
    "print(' main prediction loss: %.2f' % df_results1.main_prediction_loss.mean())\n",
    "print(' frequency prediction loss: %.2f' % df_results1.frequency_prediction_loss.mean())\n",
    "print(' sound level prediction loss: %.2f' % df_results1.sl_prediction_loss.mean())\n",
    "print()\n",
    "print('main prediction accuracy: %.2f' % df_results1.main_prediction_acc.mean())"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
