{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook is used to train the **first convolutional neural network** of the automated ABR thresholder with data provided by [Ingham et. al](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000194). \n",
    "\n",
    "The first model (**Model I**) is trained as classifier to predict if an ABR response is present or not present in a single stimulus curve - one frequency, one sound pressure level (SPL). The required labels for Model I are derived from the original hearing thresholds under the assumption that all sub-threshold SPL curves represent non-hearing, while threshold and supra-threshold SPL curves represent hearing.\n",
    "\n",
    "The input data consists of the measured ABR curves represented as time series and the SPL associated with each curve.\n",
    "\n",
    "The prediction values of Model I are used as input data for the second neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T08:10:49.717560Z",
     "start_time": "2021-02-26T08:10:49.484339Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T08:10:59.071095Z",
     "start_time": "2021-02-26T08:10:59.050625Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T08:11:06.012744Z",
     "start_time": "2021-02-26T08:11:05.053088Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 16]\n",
    "\n",
    "import ABR_ThresholdFinder_NN.data_preparation as dataprep\n",
    "from ABR_ThresholdFinder_NN.models import create_model_1, compile_model_1\n",
    "\n",
    "\"\"\"Set the available GPUs\"\"\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "if os.environ[\"CUDA_VISIBLE_DEVICES\"]:\n",
    "    count_gpus = len([int(s) for s in os.environ['CUDA_VISIBLE_DEVICES'].split(',')])\n",
    "    print('%d GPUs available' % count_gpus)\n",
    "else:\n",
    "    count_gpus = 0\n",
    "    print('no GPUs available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Definitions\n",
    "\n",
    "Define variables and methods to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:30:31.329783Z",
     "start_time": "2021-02-04T13:30:31.272789Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Set the batch size\"\"\"\n",
    "batch_size = 32\n",
    "print('batch size %d' % batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Define potential frequencies measured in Hz, with the exception of 100, \n",
    "which stands for a broadband frequency stimulus (click)  \n",
    "\"\"\"\n",
    "poss_freq = [100, 6000, 12000, 18000, 24000, 30000]\n",
    "print(*['potential stimulus frequencies: ' + str(x) if x==100 else str(x)+'Hz' for x in poss_freq], sep = \", \") \n",
    "\n",
    "\"\"\"Define potential sound pressure levels measured in dB\"\"\"\n",
    "poss_thr = [p for p in range(0, 100, 5)]\n",
    "print(*['potential sound pressure levels [dB]: ' + str(x) if x==0 else str(x) for x in poss_thr], sep = \", \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:30:34.522169Z",
     "start_time": "2021-02-04T13:30:34.152694Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Set the path to the data files, for example '../data/ING/'\"\"\"\n",
    "data_dir = ''\n",
    "\"\"\"Define the name of the file containing the ABR curve dataset\"\"\"\n",
    "data_file = os.path.join(data_dir, 'ING_abr_curves.csv')\n",
    "if os.path.exists(data_file):\n",
    "    print('data file: {}'.format(data_file))\n",
    "else: \n",
    "    print('ERROR: data file not found!')\n",
    "\"\"\"Define the folder in which to store the trained models\"\"\"\n",
    "save_dir = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:31:55.860886Z",
     "start_time": "2021-02-04T13:30:36.387909Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the data and add a column for (non-)hearing definition of an ABR curve\"\"\"\n",
    "data = pd.read_csv(data_file, low_memory=True)\n",
    "data['still_hears'] = [1 if data.loc[idx, 'sound_level'] >= data.loc[idx, 'threshold'] else 0 for idx in data.index]\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:32:20.466842Z",
     "start_time": "2021-02-04T13:32:14.847746Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Prepare the data to feed the neural network\"\"\"\n",
    "data1 = dataprep.prepare_data4training_1(data, poss_freq, poss_thr, _ING_model=True)\n",
    "display(data1.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:32:25.152844Z",
     "start_time": "2021-02-04T13:32:25.092268Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The input data has to fulfill certain requirements: \n",
    "- define [\"still_hears\"] as main_label_col\n",
    "- only the first 1000 columns (time steps) are to be considered \n",
    "\"\"\"\n",
    "main_label_col, freq_label_col, sl_label_col, data_cols = dataprep.get_col_names4model_1_labels(data1)\n",
    "print('main label column %s' % main_label_col)\n",
    "print('frequency label columns: [%s ... %s]' % (freq_label_col[0], freq_label_col[-1]))\n",
    "print('sound level label columns: [%s ... %s]' % (sl_label_col[0], sl_label_col[-1]))\n",
    "print('data columns: [%s ... %s]' % (data_cols[0], data_cols[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New split of the training data in training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "print('random seed: {}'.format(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split all data into train data and valid data (80:20)\"\"\"\n",
    "train_mice, valid_mice, train_indices, valid_indices = dataprep.split_data(data1, random_seed) \n",
    "print('train index: %d' % len(train_indices))\n",
    "print('valid index: %d' % len(valid_indices))\n",
    "\n",
    "train_data = data1.loc[train_indices]\n",
    "train_data.head(2)\n",
    "\n",
    "\"\"\"Split train data into train data 2 and test data (80:20)\"\"\"\n",
    "train_mice2, test_mice, train_indices2, test_indices = dataprep.split_data(train_data, random_seed)\n",
    "train_data2 = train_data.loc[train_indices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(data_dir,'ING_train_mice.npy')\n",
    "np.save(data_file, train_mice2)\n",
    "data_file = os.path.join(data_dir,'ING_valid_mice.npy')\n",
    "np.save(data_file, valid_mice)\n",
    "data_file = os.path.join(data_dir,'ING_test_mice.npy')\n",
    "np.save(data_file, test_mice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the existing training, validation and test data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:32:29.481921Z",
     "start_time": "2021-02-04T13:32:29.397748Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mice = np.load(os.path.join(data_dir, 'ING_train_mice.npy'), allow_pickle=True)\n",
    "valid_mice = np.load(os.path.join(data_dir, 'ING_valid_mice.npy'), allow_pickle=True)\n",
    "test_mice = np.load(os.path.join(data_dir, 'ING_test_mice.npy'), allow_pickle=True)\n",
    "\n",
    "train_indices = data1.index[data1['mouse_id'].isin(train_mice)]\n",
    "valid_indices = data1.index[data1['mouse_id'].isin(valid_mice)]\n",
    "test_indices = data1.index[data1['mouse_id'].isin(test_mice)]\n",
    "\n",
    "print('train index: %d' % len(train_indices))\n",
    "print('valid index: %d' % len(valid_indices))\n",
    "print('test index: %d' % len(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:32:58.383180Z",
     "start_time": "2021-02-04T13:32:46.004051Z"
    }
   },
   "outputs": [],
   "source": [
    "data2 = dataprep.standardize(data1, train_indices, data_cols)\n",
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:33:23.392545Z",
     "start_time": "2021-02-04T13:33:23.338478Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Import the data generator\"\"\"\n",
    "import ABR_ThresholdFinder_NN.data_generator as datagenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:33:29.855304Z",
     "start_time": "2021-02-04T13:33:29.788834Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the data generators (actually keras sequences)\"\"\"\n",
    "train_data_generator = datagenerate.DataGenerator(list_IDs=train_indices2, \n",
    "                                     df=data2, \n",
    "                                     value_cols=data_cols, \n",
    "                                     main_label_col=main_label_col, \n",
    "                                     freq_label_col=freq_label_col, \n",
    "                                     sl_label_col=sl_label_col,\n",
    "                                     dim=1000, \n",
    "                                     batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "valid_data_generator = datagenerate.DataGenerator(list_IDs=valid_indices, \n",
    "                                     df=data2, \n",
    "                                     value_cols=data_cols, \n",
    "                                     main_label_col=main_label_col,\n",
    "                                     freq_label_col=freq_label_col, \n",
    "                                     sl_label_col=sl_label_col,\n",
    "                                     dim=1000, \n",
    "                                     batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "test_data_generator = datagenerate.DataGenerator(list_IDs=test_indices, \n",
    "                                    df=data2, \n",
    "                                    value_cols=data_cols, \n",
    "                                    main_label_col=main_label_col,\n",
    "                                    freq_label_col=freq_label_col, \n",
    "                                    sl_label_col=sl_label_col, \n",
    "                                    dim=1000, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:33:35.682863Z",
     "start_time": "2021-02-04T13:33:34.963999Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create Model I\"\"\"\n",
    "model = create_model_1(len(freq_label_col), len(sl_label_col))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:33:42.242950Z",
     "start_time": "2021-02-04T13:33:39.358645Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Compile Model I\"\"\"\n",
    "parallel_model, loss, loss_weights = compile_model_1(model, count_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:33:46.076410Z",
     "start_time": "2021-02-04T13:33:46.017515Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Import callbacks\"\"\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T18:41:03.916972Z",
     "start_time": "2021-02-04T13:34:24.839565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define metric for early stopping\"\"\"\n",
    "mon_var = 'val_main_prediction_loss'\n",
    "\n",
    "\"\"\"Define callbacks\"\"\"\n",
    "checkpointer = ModelCheckpoint(filepath=save_dir + 'INGtrained_model_1_weights.hdf5', \n",
    "                               verbose=1, \n",
    "                               save_best_only=True, \n",
    "                               monitor=mon_var)\n",
    "early_stopper = EarlyStopping(monitor=mon_var, patience=7)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=mon_var, patience=2)\n",
    "\n",
    "\"\"\"Training\"\"\"\n",
    "history = parallel_model.fit_generator(generator=train_data_generator,\n",
    "                                       validation_data=valid_data_generator, \n",
    "                                       use_multiprocessing=True, \n",
    "                                       epochs=30,\n",
    "                                       workers=32,\n",
    "                                       shuffle=True,\n",
    "                                       callbacks=[checkpointer, early_stopper, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T18:49:08.256482Z",
     "start_time": "2021-02-04T18:49:07.438459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Summarize history for loss\"\"\"\n",
    "plt.figure(figsize=(20, 14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history.history['main_prediction_loss'])\n",
    "plt.plot(history.history['val_main_prediction_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig(save_dir + \"INGtrained_model_1_loss_history.png\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Summarize history for accuracy\"\"\"\n",
    "plt.figure(figsize=(20, 14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history.history['main_prediction_acc'])\n",
    "plt.plot(history.history['val_main_prediction_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.savefig(save_dir + \"INGtrained_model_1_acc_history.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:31:57.690380Z",
     "start_time": "2021-02-04T20:22:01.816368Z"
    }
   },
   "outputs": [],
   "source": [
    "results = parallel_model.evaluate(test_data_generator)\n",
    "results = dict(zip(parallel_model.metrics_names,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:20:54.992704Z",
     "start_time": "2021-02-05T06:20:54.937903Z"
    }
   },
   "outputs": [],
   "source": [
    "for metric in results: \n",
    "    print('%s: %.2f' % (metric, results[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save non-GPU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:21:11.080810Z",
     "start_time": "2021-02-05T06:21:08.325996Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(save_dir + 'INGtrained_model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:21:17.963995Z",
     "start_time": "2021-02-05T06:21:16.084660Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the model for predictions and validation\"\"\"\n",
    "from tensorflow.keras.models import load_model\n",
    "parallel_model = load_model(save_dir + 'INGtrained_model_1.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T16:10:07.368472Z",
     "start_time": "2021-02-02T16:00:42.936222Z"
    }
   },
   "source": [
    "df = parallel_model.predict(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:34:39.882709Z",
     "start_time": "2021-02-05T06:34:37.470585Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = data2[data2.mouse_id.isin(test_mice)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:35:03.751457Z",
     "start_time": "2021-02-05T06:34:43.202059Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Make predictions for test data\"\"\"\n",
    "df = parallel_model.predict(test_data[data_cols].values[:,:,np.newaxis])\n",
    "predictions = pd.concat([pd.DataFrame(df[0]), pd.DataFrame(df[1])], axis=1, ignore_index=True)\n",
    "predictions = pd.concat([predictions, pd.DataFrame(df[2])], axis=1, ignore_index=True)\n",
    "\n",
    "\"\"\"Define column names of predictions\"\"\"\n",
    "predictions.columns = main_label_col + freq_label_col + sl_label_col\n",
    "\n",
    "display(predictions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:36:06.116281Z",
     "start_time": "2021-02-05T06:36:05.615122Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Extract predicted labels\"\"\"\n",
    "\"\"\"still hearing label\"\"\"\n",
    "test_data['predicted_hears'] = round(predictions[main_label_col])\n",
    "test_data['predicted_hears_exact'] = predictions[main_label_col]\n",
    "\n",
    "\"\"\"frequency label\"\"\"\n",
    "test_data['predicted_freq_encoded'] = predictions.loc[:, freq_label_col].idxmax(axis=1)\n",
    "test_data['predicted_freq'] = test_data['predicted_freq_encoded'].str.replace(r'\\D+', '').astype('int')\n",
    "freq_dict = dict(zip(poss_freq, list(range(len(poss_freq)))))\n",
    "inv_freq_dict = {v: k for k, v in freq_dict.items()}\n",
    "test_data['predicted_freq'] = test_data['predicted_freq'].map(inv_freq_dict)\n",
    "\n",
    "\"\"\"sound level label\"\"\"\n",
    "test_data['predicted_sl_encoded'] = predictions.loc[:, sl_label_col].idxmax(axis=1)\n",
    "test_data['predicted_sl'] = test_data['predicted_sl_encoded'].str.replace(r'\\D+', '').astype('int')\n",
    "sl_dict = dict(zip(poss_thr, list(range(len(poss_thr)))))\n",
    "inv_sl_dict = {v: k for k, v in sl_dict.items()}\n",
    "test_data['predicted_sl'] = test_data['predicted_sl'].map(inv_sl_dict)\n",
    "\n",
    "display(test_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:36:22.966541Z",
     "start_time": "2021-02-05T06:36:18.382894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Accuracy predictions\"\"\"\n",
    "test_data1 = test_data.copy()\n",
    "\n",
    "freq_acc = 100 * round(len(test_data1[test_data1['frequency'] == test_data1['predicted_freq']]) / len(test_data1), 3)\n",
    "print('frequency accuracy: %.2f%%' % freq_acc)\n",
    "print()\n",
    "    \n",
    "sl_acc = 100 * round(len(test_data1[test_data1['sound_level'] == test_data1['predicted_sl']]) / len(test_data1), 3)\n",
    "print('sound level accuracy: %.2f%%' % sl_acc)\n",
    "print()\n",
    "    \n",
    "curve_acc = 100 * round(len(test_data1[test_data1['still_hears'] == test_data1['predicted_hears']]) / len(test_data1), 4)\n",
    "print('curve accuracy: %.2f%%' % curve_acc)\n",
    "print()\n",
    "\n",
    "print('frequency specific curve accuracy (count of test data)')\n",
    "for f in test_data1['frequency'].unique():\n",
    "    freq_spec_curve_acc = 100 * round(len(test_data1[(test_data1['still_hears'] == test_data1['predicted_hears']) & \n",
    "                                                     (test_data1['frequency'] == f)]) / \n",
    "                                      len(test_data1[test_data1['frequency'] == f]), 3)\n",
    "    count_of_test_data = len(test_data1[test_data1['frequency'] == f])\n",
    "    if f == 100:\n",
    "        print(' click: %.2f%% (%d)' % (freq_spec_curve_acc, count_of_test_data))\n",
    "    else:\n",
    "        print(' frequency %d: %.2f%% (%d)' % (f, freq_spec_curve_acc, count_of_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot of prediction probabilities of hearing and non-hearing curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:38:05.650982Z",
     "start_time": "2021-02-05T06:38:02.845479Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Visualize prediction probabilites of hearing and non-hearing curves\"\"\"\n",
    "still_hears = [0, 1]\n",
    "plt.figure(figsize=(20, 14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "\"\"\"Iterate through the two hearing possibilities\"\"\"\n",
    "for still_hearing in still_hears:\n",
    "    subset = test_data1[test_data1['still_hears'] == still_hearing]\n",
    "    sns.kdeplot(subset['predicted_hears_exact'], shade=True, gridsize=70, label=still_hearing)\n",
    "\n",
    "\"\"\"Plot formatting\"\"\"\n",
    "plt.legend(prop={'size': 16}, title = 'predicted hears')\n",
    "plt.title('Density Plot hearing prediction values')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store predictions to be fed into the second neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:40:32.441464Z",
     "start_time": "2021-02-05T06:38:27.442737Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create new dataframe for predicting\"\"\"\n",
    "test_data2 = data2.copy() # data2 entspricht komplettem Datensatz; hier test_data2 genommen, um oben verwendetes test_data1 nicht zu Ã¼berschreiben\n",
    "\n",
    "\"\"\"Make predictions for all data\"\"\"\n",
    "predictions_temp = parallel_model.predict(test_data2[data_cols].values[:,:,np.newaxis])\n",
    "predictions = pd.concat([pd.DataFrame(predictions_temp[0]), pd.DataFrame(predictions_temp[1])], axis=1, ignore_index=True)\n",
    "predictions = pd.concat([predictions, pd.DataFrame(predictions_temp[2])], axis=1, ignore_index=True)\n",
    "\n",
    "\"\"\"Define column names of predictions\"\"\"\n",
    "predictions.columns = main_label_col + freq_label_col + sl_label_col\n",
    "predictions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:41:59.860590Z",
     "start_time": "2021-02-05T06:41:57.636773Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Extract predicted labels\"\"\"\n",
    "\"\"\"still hearing label\"\"\"\n",
    "test_data2['predicted_hears'] = round(predictions[main_label_col])\n",
    "test_data2['predicted_hears_exact'] = predictions[main_label_col]\n",
    "\n",
    "\"\"\"frequency label\"\"\"\n",
    "test_data2['predicted_freq_encoded'] = predictions.loc[:, freq_label_col].idxmax(axis=1)\n",
    "test_data2['predicted_freq'] = test_data2['predicted_freq_encoded'].str.replace(r'\\D+', '').astype('int')\n",
    "inv_freq_dict = {v: k for k, v in freq_dict.items()}\n",
    "test_data2['predicted_freq'] = test_data2['predicted_freq'].map(inv_freq_dict)\n",
    "\n",
    "\"\"\"sound level label\"\"\"\n",
    "test_data2['predicted_sl_encoded'] = predictions.loc[:, sl_label_col].idxmax(axis=1)\n",
    "test_data2['predicted_sl'] = test_data2['predicted_sl_encoded'].str.replace(r'\\D+', '').astype('int')\n",
    "inv_sl_dict = {v: k for k, v in sl_dict.items()}\n",
    "test_data2['predicted_sl'] = test_data2['predicted_sl'].map(inv_sl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:42:42.143200Z",
     "start_time": "2021-02-05T06:42:10.511337Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Write csv of exact predictions for external use\"\"\"\n",
    "test_data2.loc[test_data2['mouse_id'].isin(train_mice), 'mouse_group'] = 'train'\n",
    "test_data2.loc[test_data2['mouse_id'].isin(valid_mice), 'mouse_group'] = 'valid'\n",
    "test_data2.loc[test_data2['mouse_id'].isin(test_mice), 'mouse_group'] = 'test'\n",
    "\n",
    "\"\"\"'validated' entfernt, da nicht vorhanden\"\"\"\n",
    "test_data2[['mouse_id', 'mouse_group', 'frequency', 'threshold', 'sound_level', \n",
    "            'predicted_hears_exact']].to_csv(save_dir + 'model_1_predicted_curves_21_04.csv', index=False)\n",
    "\n",
    "print(test_data2[['mouse_id', 'mouse_group', 'frequency', 'threshold', 'sound_level']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:43:48.244852Z",
     "start_time": "2021-02-05T06:43:48.187568Z"
    }
   },
   "outputs": [],
   "source": [
    "data_file = os.path.join(save_dir, 'INGtrained_model_1_predicted_curves.csv')\n",
    "os.path.exists(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:43:54.756797Z",
     "start_time": "2021-02-05T06:43:54.501079Z"
    }
   },
   "outputs": [],
   "source": [
    "all_pred_curves = pd.read_csv(data_file)\n",
    "all_pred_curves.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:44:20.157649Z",
     "start_time": "2021-02-05T06:44:09.203406Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "for mouse in all_pred_curves.mouse_id.unique()[:10]:\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(15, 8))\n",
    "\n",
    "    df = all_pred_curves[all_pred_curves.mouse_id == mouse]\n",
    "    \n",
    "    ncols = 3\n",
    "    nrows = int(len(df.frequency.unique())/ncols)\n",
    "    col = 0\n",
    "    row = 0\n",
    "    spec = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n",
    "    f_ax = {}\n",
    "    for idx,freq in enumerate(df.frequency.unique()):\n",
    "        f_ax[idx] = fig.add_subplot(spec[row, col])\n",
    "        if freq == 100: \n",
    "            f_ax[idx].set_title('Click')\n",
    "        else:\n",
    "            f_ax[idx].set_title('%sHz' % freq)\n",
    "        f_ax[idx].set_xticks(df.sound_level.unique())\n",
    "        df[df.frequency == freq].plot(x='sound_level', y='predicted_hears_exact', ax=f_ax[idx], legend=False)\n",
    "        plt.vlines(x=df[df.frequency == freq]['threshold'], ymin=0, ymax=1., linestyles='dashed', color='lightgray')\n",
    "        col+=1\n",
    "        if col == ncols:\n",
    "            row+=1\n",
    "            col=0\n",
    "    fig.suptitle('%s mouse id: %s' % (df.mouse_group.unique()[0], mouse), fontsize=16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T06:46:11.287961Z",
     "start_time": "2021-02-05T06:46:06.115636Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "noofmice = 10\n",
    "\n",
    "for group in all_pred_curves.mouse_group.unique():\n",
    "    \n",
    "    df = all_pred_curves.loc[all_pred_curves.mouse_group == group]\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(15, 8))\n",
    "\n",
    "    ncols = 3\n",
    "    nrows = int(len(df.frequency.unique())/ncols)\n",
    "    col = 0\n",
    "    row = 0\n",
    "    spec = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n",
    "    f_ax = {}\n",
    "\n",
    "    for idx,freq in enumerate(df.frequency.unique()):\n",
    "        \n",
    "        df1 = df[df.frequency == freq]\n",
    "    \n",
    "        f_ax[idx] = fig.add_subplot(spec[row, col])\n",
    "        if freq == 100:\n",
    "            f_ax[idx].set_title('Click')\n",
    "        else:\n",
    "            f_ax[idx].set_title('%sHz' % freq)\n",
    "        f_ax[idx].set_xticks(df1.sound_level.unique())\n",
    "        \n",
    "        for mouse in df1.mouse_id.unique()[:noofmice]:\n",
    "            df1.loc[df1.mouse_id == mouse].plot(x='sound_level', y='predicted_hears_exact', ax=f_ax[idx], legend=False)\n",
    "            \n",
    "        col+=1\n",
    "        if col == ncols:\n",
    "            row+=1\n",
    "            col=0\n",
    "           \n",
    "    fig.suptitle('mouse group: %s (%d mice)' % (group, noofmice), fontsize=16)  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "266.85px",
    "left": "1237px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}